<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>grafana | shira.at</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/style.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">grafana | shira.at</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#about-this-page" id="toc-about-this-page">About this
page</a></li>
<li><a href="#my-experience" id="toc-my-experience">My
experience</a></li>
<li><a href="#inner-workings-of-grafana-monitoring"
id="toc-inner-workings-of-grafana-monitoring">Inner workings of grafana
monitoring</a></li>
<li><a href="#installing" id="toc-installing">Installing</a>
<ul>
<li><a href="#grafana" id="toc-grafana">Grafana</a>
<ul>
<li><a href="#grafana-behind-reverse-proxy"
id="toc-grafana-behind-reverse-proxy">Grafana behind reverse
proxy</a></li>
</ul></li>
<li><a href="#node-exporter" id="toc-node-exporter">Node
exporter</a></li>
<li><a href="#mysqld_exporter"
id="toc-mysqld_exporter">mysqld_exporter</a></li>
<li><a href="#prometheus" id="toc-prometheus">Prometheus</a></li>
</ul></li>
<li><a href="#grafana-data-sources"
id="toc-grafana-data-sources">Grafana data sources</a></li>
<li><a href="#monitoring-golang-gin-webservers"
id="toc-monitoring-golang-gin-webservers">Monitoring golang gin
webservers</a></li>
<li><a href="#create-alerts-for-downtimes"
id="toc-create-alerts-for-downtimes">Create alerts for
downtimes</a></li>
<li><a href="#creating-histogram-graphs-in-grafana"
id="toc-creating-histogram-graphs-in-grafana">Creating histogram graphs
in grafana</a></li>
<li><a href="#installing-loki-and-promtail"
id="toc-installing-loki-and-promtail">Installing Loki and
Promtail</a></li>
<li><a href="#opinion-about-grafana-loki"
id="toc-opinion-about-grafana-loki">Opinion about grafana loki</a>
<ul>
<li><a href="#my-old-opinion-about-grafana-loki"
id="toc-my-old-opinion-about-grafana-loki">My old opinion about grafana
loki</a></li>
</ul></li>
</ul>
</nav>
<p>Last changed: 2023.07.01</p>
<h1 id="about-this-page">About this page</h1>
<p>This page will explain how to set up grafana based monitoring on a
single linux server without any docker or cloud usage.<br />
In my opinion you don’t need a kubernetes setup for a single server, but
the documentation is lacking in explaining on how to set it up on “bare
metal linux”.</p>
<p>This page will explain on how to install it from standalone linux
binaries only. This is quite easy to automate, setup and run.</p>
<h1 id="my-experience">My experience</h1>
<p>I use node_exporter, mysql_exporter, prometheus and grafana - and
none of those ever crashed. They are running for close to a year now
without any problems. I started them inside a “screen” and since then
they never died on me.</p>
<p>I have the default config of 14 days for data retention in
prometheus, and all of my above mentioned applications plus 2 other
golang web applications use around 2.3GB of storage space. This is quite
acceptable, as it will never get higher than this.</p>
<h1 id="inner-workings-of-grafana-monitoring">Inner workings of grafana
monitoring</h1>
<p>A short summary for those who don’t know how grafana and prometheus
work.</p>
<p>Grafana is <em>only</em> the front end and prometheus is the storage
backend.<br />
Prometheus pulls metrics every 15 seconds (by default) and saves them.
This means you don’t send metrics but instead prometheus gets them from
your application. This also means that your application has to expose a
HTTP webserver where prometheus can get the metrics from.<br />
These metrics, which are normally exposed at /metrics
(e.g. http://localhost:8080/metrics), are just a key-value pairing and
the values are all numbers.</p>
<p>An example for metrics are:</p>
<pre><code># TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 8.59
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 9</code></pre>
<p>The first one of those metrics is a counter. This counter can only go
up (by e.g. counter.Inc()) and measures e.g. total http requests
served.<br />
The second metric is a gauge, a number that can go up and down. This for
example measures e.g. concurrent http requests being handled right
now.</p>
<p>Prometheus is a time series database, which means that Grafana (the
frontend) can display these 15s interval data in a graph.</p>
<p>The XXX_exporter tools mentioned in the first paragraph are tools
that monitor your e.g. linux system or mariadb database. These are
premade services that enable you to easily monitor everything about your
services, e.g. with linux the “open file descriptors” or “ram
usage”.</p>
<h1 id="installing">Installing</h1>
<p>Before installing please make sure that your firewall config is up to
date.<br />
Your exporters, grafana and prometheus do not have to be available from
the internet, so block all ports you dont need.</p>
<p>In my setup I only have port 443 (HTTPS) open for grafana, because
all other applications run on the same server so no port has to be
opened.</p>
<h2 id="grafana">Grafana</h2>
<p>Grafana is the UI frontend of the whole monitoring.</p>
<p>You can download it from here: <a
href="https://grafana.com/grafana/download?pg=get&amp;plcmt=selfmanaged-box1-cta1&amp;edition=oss">https://grafana.com/grafana/download?pg=get&amp;plcmt=selfmanaged-box1-cta1&amp;edition=oss</a></p>
<p>To download it you can use</p>
<pre><code>wget https://dl.grafana.com/oss/release/grafana-10.0.1.linux-amd64.tar.gz
tar -zxvf grafana-10.0.1.linux-amd64.tar.gz</code></pre>
<p>To start it you can create a “startgrafana.sh” file containing
this:</p>
<pre><code>./grafana-10.0.1/bin/grafana server --homepath &quot;grafana-10.0.1&quot; web</code></pre>
<p>Grafana will run on port 3000 and the default login is
admin/admin.</p>
<p>If you forget your password later you can use the following method to
reset it back to admin/admin:</p>
<pre><code>cd grafana-10.0.1
bin/grafana cli admin reset-admin-password admin</code></pre>
<h3 id="grafana-behind-reverse-proxy">Grafana behind reverse proxy</h3>
<p>Grafana now runs on port 3000, but it would be better to run it
behind a reverse proxy.</p>
<p>I personally use nginx so I did the following changes to be able to
use grafana behind it.</p>
<p>Create a new file called “grafana.ini” and put the following
configuration in it:</p>
<pre><code>[server]
domain = example.com
root_url = https://example.com/grafana/
serve_from_sub_path = false</code></pre>
<p><strong>Attention</strong>: With Grafana 10 you have to set
<code>serve_from_sub_path=false</code> but with Grafana 9 you have to
set this to true.</p>
<p>Then change your “startgrafana.sh” file to add this config:</p>
<pre><code>./grafana-10.0.1/bin/grafana server --config &quot;grafana.ini&quot; --homepath &quot;grafana-10.0.1&quot; web</code></pre>
<p>This way grafana knows that it’s being called by a reverse proxy
subpath and all the http connections will work.</p>
<p>For the nginx config I used the following to serve it in the “grana”
subpath:</p>
<pre><code>#grafana
location /grafana/ {
    rewrite ^/grafana/(.*) /$1 break;
    proxy_set_header Host $http_host;
    proxy_pass http://localhost:3000;
}
location /grafana/api/live/ {
    rewrite ^/grafana/(.*) /$1 break;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection $connection_upgrade;
    proxy_set_header Host $http_host;
    proxy_pass http://localhost:3000;
}</code></pre>
<p>After restarting nginx you should be able to access grafana through
your nginx instance.</p>
<p>My folder for grafana looks like this now:</p>
<pre><code>$ ls -al
..
grafana-10.0.1
grafana.ini
startgrafana.sh</code></pre>
<p>To run grafana “forever” I simply started a screen
(<code>screen -S grafana</code>) and inside that started the sh script
(<code>./startgrafana.sh</code>).</p>
<h2 id="node-exporter">Node exporter</h2>
<p>The node_exporter exports your linux server metrics like cpu usage,
disk usage and ram usage.</p>
<p>Download it from here: <a
href="https://github.com/prometheus/node_exporter/releases">https://github.com/prometheus/node_exporter/releases</a></p>
<p>To download it you can use:</p>
<pre><code>wget https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz
tar -xvf node_exporter-1.6.0.linux-amd64.tar.gz</code></pre>
<p>Then simply start it with:</p>
<pre><code>./node_exporter-1.6.0.linux-amd64/node_exporter</code></pre>
<p>It now listens on port 9100 and automatically serves your linux
server metrics.</p>
<p>I ran this the same way as grafana: Create a screen and run it inside
it.</p>
<h2 id="mysqld_exporter">mysqld_exporter</h2>
<p>The mysqld_exporter exports metrics about your databases’ health and
unoptimized queries.</p>
<p>Download it from here: <a
href="https://github.com/prometheus/mysqld_exporter">https://github.com/prometheus/mysqld_exporter</a></p>
<p>To download it you can use:</p>
<pre><code>wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.15.0/mysqld_exporter-0.15.0.linux-amd64.tar.gz
tar -xvf mysqld_exporter-0.15.0.linux-amd64.tar.gz</code></pre>
<p>For this to work you need to create a database user with special
permissions like this:</p>
<pre><code>CREATE USER &#39;exporter&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;XXXXXXXX&#39; WITH MAX_USER_CONNECTIONS 3;
GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO &#39;exporter&#39;@&#39;localhost&#39;;</code></pre>
<p>Then create a config file for the exporter,
e.g. “mysqld_exporter.cnf” and put your user and password of the db user
into it:</p>
<pre><code>[client]
user=exporter
password=XXXXXXXXXXXXXXXX</code></pre>
<p>To start it I use this quite full shell script named “startmysql.sh”,
which tells the program to export nearly everything:</p>
<pre><code>./mysqld_exporter-0.15.0.linux-amd64/mysqld_exporter-main --config.my-cnf mysqld_exporter.cnf --collect.global_status --collect.info_schema.innodb_metrics --collect.auto_increment.columns --collect.info_schema.processlist --collect.binlog_size --collect.info_schema.tablestats --collect.global_variables --collect.info_schema.query_response_time --collect.info_schema.userstats --collect.info_schema.tables --collect.perf_schema.tablelocks --collect.perf_schema.file_events --collect.perf_schema.eventswaits --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits</code></pre>
<p>My folder now looks like this:</p>
<pre><code>$ ls -al
..
mysqld_exporter-0.15.0.linux-amd64
mysqld_exporter.cnf
startmysql.sh</code></pre>
<p>To run it I simply create a <code>screen</code> again and run it with
<code>./startmysql.sh</code> inside it.</p>
<h2 id="prometheus">Prometheus</h2>
<p>Now to start the actual monitoring. Prometheus scrapes your exporters
/ metrics every 15 seconds and saves the data in its time series
database.</p>
<p>Download it from here: <a
href="https://prometheus.io/download/">https://prometheus.io/download/</a></p>
<p>You can download it with:</p>
<pre><code>wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar -xvf prometheus-2.45.0.linux-amd64.tar.gz</code></pre>
<p>Prometheus has its entire scrape config (aka. which targets it pulls
metrics from) in a single YAML file.</p>
<p>We currently have 4 scrape targets we can use:</p>
<ul>
<li>The node exporter with linux metrics, running on port 9100</li>
<li>The mysql exporter, running on port 9104</li>
<li>Grafana itself has metrics, it runs on port 3000</li>
<li>Prometheus can also scrape itself, it runs on port 9090</li>
</ul>
<p>This means our prometheus.yml file looks like this:</p>
<pre><code>global:
  scrape_interval:     15s
  evaluation_interval: 15s
scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets: [&#39;localhost:9090&#39;]
  - job_name: grafana
    static_configs:
      - targets: [&#39;localhost:3000&#39;]
  - job_name: linux
    static_configs:
      - targets: [&#39;localhost:9100&#39;]
  - job_name: mariadb
    static_configs:
      - targets: [&#39;localhost:9104&#39;]</code></pre>
<p>By default prometheus scrapes the /metrics route of all the targets
configured.</p>
<p>To now start prometheus I have a simple “startprom.sh” script that
contains:</p>
<pre><code>./prometheus-2.45.0.linux-amd64/prometheus --config.file=prometheus.yml</code></pre>
<p>My folder looks like this now: $ ls -al ..
prometheus-2.45.0.linux-amd64 prometheus.yml startprom.sh</p>
<p>To start it I run it in a simple <code>screen</code> with
<code>./startprom.sh</code>.</p>
<h1 id="grafana-data-sources">Grafana data sources</h1>
<p>Now that prometheus and grafana are running you can add prometheus as
your data source in grafana.</p>
<p>In Grafana v9 go to the bottom left corner Administration -&gt; Data
sources, in Grafana v10 go to the top left and Administration -&gt; Data
sources.</p>
<p>Click on add new one and Prometheus.<br />
Here you can simply enter the default “http://localhost:9090” Prometheus
server url and it should work.<br />
Click on “Save &amp; test” at the bottom.</p>
<p>Now you can, for example, go to the left side “Explore” section and
explore the Prometheus data source. There should be a lot of metrics
already, e.g. go_mestats_mallocs_total.</p>
<p>You can also add your database as a data source. This way you can
execute database queries and display them on your dashboard. This is not
recommended by security though, as you could run INSERT or DELETE
queries this way and changing production data from your monitoring is
not the best security practise.</p>
<p>I won’t go into detail on how to create dashboards as that is a whole
new world by itself.</p>
<h1 id="monitoring-golang-gin-webservers">Monitoring golang gin
webservers</h1>
<p>Adding prometheus capabilities for metrics to your golang application
is quite easy. You simply have to build a middleware that counts your
metrics and use it first before other routes.</p>
<p>My commit for adding this to a golang gin-gonic rest server, to
easily compare what has been added: <a
href="https://github.com/OverlordAkise/istina/commit/5631dd7f70ee7c15e91d0d489f50fe5799cfa598">https://github.com/OverlordAkise/istina/commit/5631dd7f70ee7c15e91d0d489f50fe5799cfa598</a></p>
<p>The <code>promhttp</code> package by itself has only metrics about
your internal golang application, e.g. ram and cpu usage, but not about
your http webserver metrics.<br />
There may be packages out there that easily let you add support for
gin-gonic metrics, but I decided to only build in what I need.</p>
<p>An example cutout of the code commit above:</p>
<p>At the beginning you create your prometheus “counters”, e.g.:</p>
<pre><code>requestCounter := prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Namespace: &quot;myapp&quot;,
        Name:      &quot;http_requests_total&quot;,
        Help:      &quot;Number of received http requests&quot;,
    },
    []string{&quot;code&quot;, &quot;method&quot;, &quot;host&quot;, &quot;url&quot;},
)
prometheus.MustRegister(requestCounter)</code></pre>
<p>Then you create a middleware. This middleware should be created and
used before any other route or middleware has been added, so that it can
measure all of the application:</p>
<pre><code>r.Use(func(c *gin.Context) {
    c.Next()
    if c.Request.URL.Path != &quot;/metrics&quot; &amp;&amp; c.Request.URL.Path != &quot;/favicon.ico&quot; {
        requestCounter.WithLabelValues(status, c.Request.Method, c.Request.Host, c.Request.URL.Path).Inc()
    }
})</code></pre>
<p>Now, everytime a request comes in, this middleware gets called. It
lets the request through by calling c.Next() and after the request
bubbles up the stack again it increments the counter of “requestCounter”
with the specified Method/Host/Path labels.</p>
<p>To now expose this we will create a /metrics route.<br />
By default promhttp expects a default “net/http” golang route function,
with “w writer” and “r response” parameters.</p>
<p>We can do this with gin-gonic by wrapping it like this:</p>
<pre><code>r.GET(&quot;/metrics&quot;, gin.WrapH(promhttp.Handler()))</code></pre>
<p>Now your application is ready and can be scraped by prometheus.</p>
<h1 id="create-alerts-for-downtimes">Create alerts for downtimes</h1>
<p>Every prometheus metric-source has an <code>up</code> metric that is
available to view in grafana.<br />
You can view this metric in grafana by going to <code>Explore</code> on
the left and then simply typing in <code>up</code> and pressing
shift+enter.</p>
<p>This metric will be 1 if the service was able to be scraped
(=accessed) by prometheus and will be 0 if Prometheus wasn’t able to
access the /metrics path.<br />
This metric has a different label (job) for each service.</p>
<p>You can easily monitor your services (in general, if they are
running) by creating an alert with the <code>up</code> metric.<br />
On the left menu you have an “Alerting” tab. There you can create
“Contact points” (the place where you will get your is-down
notification) and “Alert rules” (the checks that send
notifications).</p>
<p>You can create an alert rule with the following:</p>
<ul>
<li>Prometheus, Time Range “now-5m to now”</li>
<li>PromQL query: <code>up</code></li>
<li>Expressions B (Reduce): Input A, Function Min, Mode Strict</li>
<li>Expressions C (Threshold): Input B, Is Below, 1</li>
</ul>
<p>This way your alert rule will check every 5 minutes if a service is
down. If it is down for 2 checks in a row (e.g. 10 minutes) then you
will get a notification.</p>
<h1 id="creating-histogram-graphs-in-grafana">Creating histogram graphs
in grafana</h1>
<p>The above commit shows a “HistogramVec”, so a bucket-based metric
with different labels.<br />
This means there are a set of buckets for every label combination. This
is nice to have for detailed analysis, but for simple graphs you can
combine all buckets into one.</p>
<p>To display this in grafana I recommend the usage of the “Bar Gauge”
type.</p>
<p>Create a new panel and set the type to “Bar gauge”.<br />
In the QL field enter your bucket metric,
e.g. “myapp_http_duration_seconds_bucket”.</p>
<p>If your code is exactly like mine, then you will see multiple “0.01”
and multiple “+inf” buckets, one for each label combination (e.g. one
set of buckets for each HTTP route).</p>
<p>This is quite big and not easily readable, so I prefer to just
combine them into one.</p>
<p>In the QL field instead enter:</p>
<pre><code>sum by (le) (myapp_http_duration_seconds_bucket)</code></pre>
<p>This way all buckets will be summarized, so you only have one “+inf”
and one “0.01” bucket displayed.</p>
<p>To change the labels open the “options” panel at the bottom and in
the “Legend” field enter <code>{{le}}</code>.</p>
<p>I have 3 main routes but my bucket bar gauge is now displayed as
one:</p>
<figure>
<img src="files/grafana_bar_gauge.png"
alt="an image of a bar gauge in grafana" />
<figcaption aria-hidden="true">an image of a bar gauge in
grafana</figcaption>
</figure>
<h1 id="installing-loki-and-promtail">Installing Loki and Promtail</h1>
<p>Loki is the logging backend that has to be added as a data source in
grafana.<br />
Promtail is the “log-grabber” that basically uses <code>tail -f</code>
to send the log files you specify to loki.</p>
<p>To install loki and promtail first download them from here: <a
href="https://github.com/grafana/loki/releases">https://github.com/grafana/loki/releases</a><br />
Example:</p>
<pre><code>wget &quot;https://github.com/grafana/loki/releases/download/v2.9.2/loki-linux-amd64.zip&quot;
wget &quot;https://github.com/grafana/loki/releases/download/v2.9.2/promtail-linux-amd64.zip&quot;
unzip loki-linux-amd64.zip
unzip promtail-linux-amd64.zip</code></pre>
<p>An example loki config, taken from <a
href="https://grafana.com/docs/loki/latest/configure/examples/#1-local-configuration-exampleyaml">https://grafana.com/docs/loki/latest/configure/examples/#1-local-configuration-exampleyaml</a><br />
This will be put into <code>loki.yaml</code> in the loki folder:</p>
<pre><code>auth_enabled: false
server:
  http_listen_port: 3100
common:
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory
  replication_factor: 1
  path_prefix: /tmp/loki
schema_config:
  configs:
  - from: 2020-05-15
    store: boltdb-shipper
    object_store: filesystem
    schema: v11
    index:
      prefix: index_
      period: 24h</code></pre>
<p>To start loki simply use the following command from within the loki
folder:</p>
<pre><code>./loki-linux-amd64 --config.file=loki.yaml</code></pre>
<p>An example promtail config, taken from <a
href="https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#example-static-config">https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#example-static-config</a><br />
This will be put into <code>promtail.yaml</code> in the promtail
folder:</p>
<pre><code>server:
  disable: true
positions:
  filename: ./positions.yaml
clients:
  - url: http://localhost:3100/loki/api/v1/push
scrape_configs:
 - job_name: myapp
   pipeline_stages:
   static_configs:
   - targets:
      - localhost
     labels:
      app: myapp
      __path__: /srv/myapp/myapp.log</code></pre>
<p>In this example configuration I changed the server to be disabled.
This server is meant to receive logs via http push, which is not needed
if you simply tail the logfiles.</p>
<p>To start promtail simply use the following command from within the
promtail folder:</p>
<pre><code>./promtail-linux-amd64 -config.file promtail.yaml</code></pre>
<p>Now all that is left is to add the loki data source in grafana, which
can be done with the url “http://localhost:3100”.</p>
<h1 id="opinion-about-grafana-loki">Opinion about grafana loki</h1>
<p>I have now used grafana loki for close to a year. It is similar to
Elasticsearch.<br />
It is still, in my opinion, not a good replacement for all the features
and performance of Elasticsearch, BUT it is a very good small scale
logging server solution for non-enterprise.</p>
<p>Promtail can handle log rotation and never had any problems with
getting and sending my application logs to loki.<br />
Loki is sometimes slow to query (if you search quickly and mulitple
times in a row) but works quite well for me, a single person using it
and with ~7 services logging to it. The fact that loki doesn’t index
text never bothered me as the queries were still fast no matter what I
searched for.</p>
<p>All in all, I can recommend using Loki and Promtail for your personal
logging needs. It uses way less RAM and CPU than e.g. Elasticsearch and
works well with my logging setup of only 7 applications. I would not
recommend it for enterprise logging with many machines.</p>
<h2 id="my-old-opinion-about-grafana-loki">My old opinion about grafana
loki</h2>
<p>Loki is not enterprise ready yet.</p>
<p>My pain points with this logging solution:</p>
<p><strong>Promtail is a must have</strong><br />
They expect you to use promtail for sending logs to loki. There is no
official solution for sending logs directly out of your application or
via other log-pushers to loki. This means another application like
promtail makes your whole landscape even more complex to manage.
Promtail also only handles files,pods and journal as a log source, which
means no stdout support as far as I know.</p>
<p><strong>Promtail can’t handle log rotation</strong><br />
According to <a
href="https://grafana.com/docs/loki/latest/clients/promtail/">https://grafana.com/docs/loki/latest/clients/promtail/</a>.<br />
If I have logging in my applications I have to rotate them out before
they become too big and clog the filesystem. But promtail, the official
tool for loki, doesn’t support this, which means it’s very difficult to
use anywhere. (This seems to, as of 2023.11.13, to work?)</p>
<p><strong>Promtails config is weird</strong><br />
This may be subjective, but having this weird config setup with a
“<strong>path</strong>” variable inside the targets-&gt;labels block to
signal what log file I want to tail is neither easy to read nor easy to
configure. I also have to add “localhost” as a target, even though
promtail can only read files from your local machine. (?!)</p>
<p><strong>Expectation of cloud usage</strong><br />
Both loki and promtail have only documentation about cloud usage.
Finding an easy to follow guide to install this locally on a single
bare-metal linux server is not easy, but for e.g. Kubernetes I would use
elasticsearch and their stack anyways.</p>
<p><strong>Loki doesn’t index text</strong><br />
Loki only indexes the labels and not the message itself.<br />
This means you have to add lots of labels for easy and quick log
searching, but loki wants you to have non-dynamic labels (e.g. a fixed
amount of possible labels) which makes this more difficult again.<br />
This means a “correlation ID” as a label doesn’t work because it is
different for every request, of which you have thousands a minute.</p>
</body>
</html>
